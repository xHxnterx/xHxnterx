import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping

def preprocess_data_for_lstm(data):
    # Extract temporal features
    data['Year'] = data.index.year
    data['Month'] = data.index.month
    data['Day'] = data.index.day

    # Create lag features for 'Close' column
    for i in range(1, 3):
        data[f'Close_Lag_{i}'] = data['Close'].shift(i)

    # Drop rows with NaN values
    data = data.dropna()

    # Split data into features (X) and target (y)
    features = data.columns.difference(['Close'])
    X, y = data[features], data['Close']

    # Use MinMaxScaler for normalization
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    return train_test_split(X_scaled, y, test_size=0.6, shuffle=False)

def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=input_shape, return_sequences=True))
    model.add(LSTM(50, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def train_lstm_model(model, X_train_reshaped, y_train, epochs, batch_size, validation_split, callbacks):
    history = model.fit(X_train_reshaped, y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_split=validation_split, callbacks=callbacks)
    return history

def plot_results(data, y_test, predictions, residuals):
    # Plot actual vs predicted prices
    plt.figure(figsize=(20, 15))
    
    plt.subplot(3, 1, 1)
    plt.plot(data.index[-len(y_test):], y_test, label='Actual Close Price', color='blue')
    plt.plot(data.index[-len(predictions):], predictions, label='Predicted Close Price', color='red')
    plt.title('Nvidia Stock Price Prediction (LSTM)')
    plt.xlabel('Date')
    plt.ylabel('Stock Price')
    plt.legend()

    # Plot residuals
    plt.subplot(3, 1, 2)
    plt.plot(data.index[-len(residuals):], residuals, label='Residuals', color='green')
    plt.axhline(y=0, color='black', linestyle='--', label='Zero Residuals')
    plt.title('Residuals Time Series')
    plt.xlabel('Date')
    plt.ylabel('Residuals')
    plt.legend()

    # Plot MSE over epochs
    plt.subplot(3, 1, 3)
    plt.plot(history.history['loss'], label='Training Loss', color='blue')
    plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
    plt.title('Model Loss Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Load data
data = pd.read_csv('https://raw.githubusercontent.com/xHxnterx/xHxnterx/main/Nvidadata')
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)

# Data preprocessing
X_train, X_test, y_train, y_test = preprocess_data_for_lstm(data)

# Reshape features for LSTM input
input_shape = (1, X_train.shape[1])
X_train_reshaped = X_train.reshape((X_train.shape[0],) + input_shape)
X_test_reshaped = X_test.reshape((X_test.shape[0],) + input_shape)

# Build LSTM model
model = build_lstm_model(input_shape)

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = train_lstm_model(model, X_train_reshaped, y_train, epochs=100, batch_size=64, validation_split=0.1, callbacks=[early_stopping])

# Make predictions on the test set
predictions = model.predict(X_test_reshaped)

# Calculate residuals
residuals = y_test - predictions.flatten()

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
print(f'Mean Squared Error: {mse}')

# Plotting the results
plot_results(data, y_test, predictions, residuals)
